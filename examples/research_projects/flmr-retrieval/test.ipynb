{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howard/anaconda3/envs/FLMR_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.flmr.modeling_flmr because of the following error (look up to see its traceback):\ncannot import name 'FLMRModelForRetrieval' from 'transformers' (/home/howard/Projects/RAVQA_V2/transformers/src/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/src/transformers/utils/import_utils.py:1364\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/FLMR_new/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/src/transformers/models/flmr/modeling_flmr.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _split_into_batches, _sort_by_length\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/examples/research_projects/flmr-retrieval/third_party/ColBERT/colbert/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Indexer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Searcher\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/examples/research_projects/flmr-retrieval/third_party/ColBERT/colbert/indexer.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_directory, print_message\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollection_indexer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encode\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mIndexer\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/examples/research_projects/flmr-retrieval/third_party/ColBERT/colbert/indexing/collection_indexer.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresidual\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResidualCodec\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcolbert\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flmr_for_indexing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FLMRModelForIndexing\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(config, collection, shared_lists, shared_queues):\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/examples/research_projects/flmr-retrieval/third_party/ColBERT/colbert/modeling/modeling_flmr_for_indexing.py:8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Tuple, Union\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FLMRModelForRetrieval, FLMRConfig\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FLMRModelForRetrieval' from 'transformers' (/home/howard/Projects/RAVQA_V2/transformers/src/transformers/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToPILImage\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FLMRQueryEncoderTokenizer, FLMRContextEncoderTokenizer\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FLMRModelForRetrieval\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/src/transformers/utils/import_utils.py:1355\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1354\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1355\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/src/transformers/utils/import_utils.py:1354\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1354\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/RAVQA_V2/transformers/src/transformers/utils/import_utils.py:1366\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1369\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.flmr.modeling_flmr because of the following error (look up to see its traceback):\ncannot import name 'FLMRModelForRetrieval' from 'transformers' (/home/howard/Projects/RAVQA_V2/transformers/src/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from transformers import FLMRQueryEncoderTokenizer, FLMRContextEncoderTokenizer\n",
    "from transformers import FLMRModelForRetrieval\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokenizer = FLMRQueryEncoderTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "Q_encoding = query_tokenizer([\"What is the capital of France?\", \"This is a test sentence.\"])\n",
    "type(query_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(res)\n",
    "# print(question_tokenizer.batch_decode(res[\"input_ids\"]))\n",
    "\n",
    "context_tokenizer = FLMRContextEncoderTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "D_encoding = context_tokenizer([\"Paris is the capital of France.\", \"Paris is not the capital of France.\",\n",
    "                                \"This is a test sentence.\", \"This is a test negative sentence.\"])\n",
    "# print(res)\n",
    "# print(context_tokenizer.batch_decode(res[\"input_ids\"]))\n",
    "\n",
    "# flmr_model = FLMRModelForRetrieval.from_pretrained(\"bert-base-uncased\", \n",
    "#                                                     query_tokenizer=query_tokenizer, \n",
    "#                                                     context_tokenizer=context_tokenizer,\n",
    "#                                                     query_concat_output_from_vision_encoder=True,\n",
    "#                                                     query_concat_output_from_text_encoder=True,\n",
    "#                                                     context_concat_output_from_vision_encoder=True,\n",
    "#                                                     context_concat_output_from_text_encoder=True,\n",
    "#                                                     )\n",
    "\n",
    "flmr_model = FLMRModelForRetrieval.from_pretrained(\"bert-base-uncased\", \n",
    "                                                    query_tokenizer=query_tokenizer, \n",
    "                                                    context_tokenizer=context_tokenizer,\n",
    "                                                    query_concat_output_from_vision_encoder=True,\n",
    "                                                    query_concat_output_from_text_encoder=True,\n",
    "                                                    context_concat_output_from_vision_encoder=False,\n",
    "                                                    context_concat_output_from_text_encoder=True,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "# Q_image_features = torch.randn(2, 768)\n",
    "# D_image_features = torch.randn(4, 768)\n",
    "# Q_pixel_values = torch.randn(2, 3, 224, 224)\n",
    "# D_pixel_values = torch.randn(4, 3, 224, 224)\n",
    "# # FLMR forward\n",
    "# # inputs = dict(\n",
    "# #     query_input_ids=Q_encoding['input_ids'],\n",
    "# #     query_attention_mask=Q_encoding['attention_mask'],\n",
    "# #     query_pixel_values=Q_pixel_values,\n",
    "# #     query_image_features=None,\n",
    "# #     context_input_ids=D_encoding['input_ids'],\n",
    "# #     context_attention_mask=D_encoding['attention_mask'],\n",
    "# #     context_pixel_values=D_pixel_values,\n",
    "# #     context_image_features=None,\n",
    "# #     query_concat_output_from_vision_encoder=True,\n",
    "# #     query_concat_output_from_text_encoder=True,\n",
    "# #     context_concat_output_from_vision_encoder=True,\n",
    "# #     context_concat_output_from_text_encoder=True,\n",
    "# # )\n",
    "# # FLMR WIT pretraining Forward\n",
    "# inputs = dict(\n",
    "#     query_input_ids=None,\n",
    "#     query_attention_mask=None,\n",
    "#     query_pixel_values=Q_pixel_values,\n",
    "#     query_image_features=None,\n",
    "#     context_input_ids=D_encoding['input_ids'],\n",
    "#     context_attention_mask=D_encoding['attention_mask'],\n",
    "#     context_pixel_values=None,\n",
    "#     context_image_features=None,\n",
    "#     query_concat_output_from_vision_encoder=True,\n",
    "#     query_concat_output_from_text_encoder=False,\n",
    "#     context_concat_output_from_vision_encoder=False,\n",
    "#     context_concat_output_from_text_encoder=True,\n",
    "# )\n",
    "\n",
    "# forward_results = flmr_model.forward(**inputs)\n",
    "# print(forward_results)\n",
    "\n",
    "flmr_model.save_pretrained(\"./test_flmr_model\")\n",
    "query_tokenizer.save_pretrained(\"./test_flmr_model/query_tokenizer\")\n",
    "context_tokenizer.save_pretrained(\"./test_flmr_model/context_tokenizer\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# input(\"save done.\")\n",
    "# flmr_model = FLMRModelForIndexing.from_pretrained(\"bert-base-uncased\", \n",
    "#                                                    query_tokenizer=query_tokenizer, \n",
    "#                                                    context_tokenizer=context_tokenizer)\n",
    "# print(\"loading finished\")\n",
    "# input()\n",
    "\n",
    "## Call ColBERT indexing to index passages\n",
    "# generate 1000 passages\n",
    "passage_contents = [\"Test sentence {}\".format(i) for i in range(1000)]\n",
    "# generate 1000 random images for each passage, convert them into jpg and save to path\n",
    "random_images = torch.randn(1000, 3, 224, 224)\n",
    "# convert to jpg with PIL\n",
    "to_img = ToPILImage()\n",
    "for i, image in enumerate(random_images):\n",
    "    image = to_img(image)\n",
    "    image.save(os.path.join(\"./test_images\", \"{}.jpg\".format(i)))\n",
    "\n",
    "image_paths = [os.path.join(\"./test_images\", \"{}.jpg\".format(i)) for i in range(1000)]\n",
    "# random image features and convert to numpy\n",
    "passage_image_features = np.random.rand(1000, 768)\n",
    "\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "\n",
    "# from models.retriever.colbert_utils import MultiModalIndexer\n",
    "from colbert import Indexer\n",
    "\n",
    "multimodal_docs = False\n",
    "\n",
    "# Launch indexer\n",
    "with Run().context(RunConfig(nranks=1, root=\".\", experiment=f\"temp_index\")):\n",
    "    nbits = 2\n",
    "    config = ColBERTConfig(\n",
    "        nbits=nbits,\n",
    "        doc_maxlen=512,\n",
    "    )\n",
    "    print(\"indexing with\", nbits, \"bits\")\n",
    "    if multimodal_docs:\n",
    "        # custom_collection = [\n",
    "        #     (passage_content, passage_image_feature, None) for passage_content, passage_image_feature in zip(passage_contents, passage_image_features)\n",
    "        # ]\n",
    "        custom_collection = [\n",
    "            (passage_content, None, image_path) for passage_content, image_path in zip(passage_contents, image_paths)\n",
    "        ]\n",
    "    else:\n",
    "        custom_collection = passage_contents\n",
    "    \n",
    "    indexer = Indexer(\n",
    "        checkpoint=\"./test_flmr_model\", \n",
    "        config=config\n",
    "    )\n",
    "    indexer.index(\n",
    "        name=f\"temp_index.nbits={nbits}\", \n",
    "        collection=custom_collection, \n",
    "        batch_size=128,\n",
    "        overwrite=True)\n",
    "    index_path = indexer.get_index()\n",
    "    del indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLMR_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
